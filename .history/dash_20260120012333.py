import streamlit as st
import pandas as pd
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.decomposition import PCA
import random

# ...existing code...

# Safety Net features (must match training)
SAFETY_NET_FEATURES = [
    'Rotational speed [rpm]',
    'Torque [Nm]',
    'Tool wear [min]',
    'Power_W',
    'Temp_Diff_K',
    'Strain_Load'
]

# load artifacts
@st.cache_data
def load_assets():
    models = {}
    models['sentry'] = joblib.load("models/sentry_pipeline.pkl")
    try:
        models['specialist'] = joblib.load("models/specialist_pipeline.pkl")
    except Exception:
        models['specialist'] = None
    # Load Safety Net (BulletproofSafetyNet) for unsupervised anomaly detection
    # New version handles scaling internally - no separate scaler needed
    try:
        models['safety_net'] = joblib.load("models/safety_net.pkl")
    except Exception:
        models['safety_net'] = None
    # Load bounds for physics checks (optional)
    try:
        models['safety_net_bounds'] = joblib.load("models/safety_net_bounds.pkl")
    except Exception:
        models['safety_net_bounds'] = None
    # prefer calibrated HDF model if present
    # no parallel HDF detector loaded here (heuristic override used instead)
    X_test = pd.read_csv("X_test.csv")
    y_test = pd.read_csv("y_test.csv").squeeze()
    summary = open("results_summary.txt").read() if st.file_uploader is None else ""
    return models, X_test, y_test, summary

models, X_test, y_test, summary = load_assets()
THRESHOLD = 0.25
TEST_MEANS = X_test.mean()
TEST_STDS = X_test.std().replace(0, 1)

st.title("Physics-Aware Failure Dashboard")

# Failure type mapping (for Specialist outputs)
FAILURE_TYPE_MAP = {
    0: ("TWF", "Tool Wear Failure: failure due to excessive tool wear causing improper machining."),
    1: ("HDF", "Heat Dissipation Failure: inadequate cooling; process temperature too close to ambient."),
    2: ("PWF", "Power Failure: excessive power output (high speed + high torque) causing failure."),
    3: ("OSF", "Overstrain Failure: worn tool under high torque leading to breakage."),
    4: ("RNF", "Random Failure: unpredictable or sensor/noise-driven failure not explained by physics features."),
}

# Quick guide in the sidebar
with st.sidebar:
    st.header("Failure Type Guide")
    for k, (code, desc) in FAILURE_TYPE_MAP.items():
        st.markdown(f"**{k} â€” {code}**: {desc}")
    
    st.divider()
    st.header("ðŸ›¡ï¸ Safety Net v2.0")
    st.markdown("""
    **Bulletproof Anomaly Detection**
    
    Three layers of defense:
    1. **Statistical Bounds** - Z-score outliers
    2. **Physics Constraints** - P=Ï„Ï‰ violations
    3. **Isolation Forest** - Multivariate patterns
    
    *ANY layer flagging = ANOMALY*
    """)
    if models.get('safety_net') is not None:
        st.success("âœ“ Safety Net Active")
    else:
        st.warning("âš  Safety Net Not Loaded")
    

# Overview
st.header("Overview & Key Metrics")
st.text(summary)

# Confusion matrix
st.header("Sentry Confusion Matrix")
sentry = models['sentry']
y_pred = sentry.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
fig, ax = plt.subplots()
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax)
ax.set_xlabel("Predicted")
ax.set_ylabel("True")
st.pyplot(fig)

# Feature importance (extract from pipeline)
st.header("Top Feature Importances (Sentry)")
est = sentry
# if pipeline, get final estimator
try:
    final = est.named_steps['estimator']
except Exception:
    final = est
if hasattr(final, "feature_importances_"):
    fi = pd.DataFrame({
        "feature": X_test.columns,
        "importance": final.feature_importances_
    }).sort_values("importance", ascending=False).head(20)
    st.bar_chart(fi.set_index("feature")["importance"])
else:
    st.write("Importance not available for this estimator.")

# =============================================================================
# SAFETY NET SECTION (v2.0 - Bulletproof)
# =============================================================================
st.header("ðŸ›¡ï¸ Safety Net v2.0: Bulletproof Anomaly Detection")

if models.get('safety_net') is not None:
    safety_net = models['safety_net']
    
    st.markdown("""
    **Three Layers of Defense:**
    1. **Layer 1 (Statistical):** Any feature > 3Ïƒ from healthy mean = ANOMALY
    2. **Layer 2 (Physics):** Power â‰  Torque Ã— Speed violations = ANOMALY  
    3. **Layer 3 (Isolation Forest):** Multivariate pattern outliers = ANOMALY
    
    *ANY layer flagging = ANOMALY (defense in depth)*
    """)
    
    # Check if all required features are available
    available_features = [f for f in SAFETY_NET_FEATURES if f in X_test.columns]
    
    if len(available_features) == len(SAFETY_NET_FEATURES):
        # BulletproofSafetyNet takes DataFrame directly, handles scaling internally
        X_safety = X_test.copy()
        
        # Get predictions using new API
        anomaly_preds = safety_net.predict(X_safety)
        anomaly_scores = safety_net.decision_function(X_safety)
        
        # Get detailed layer-by-layer breakdown
        try:
            detailed_preds = safety_net.predict_detailed(X_safety)
            has_detailed = True
        except Exception:
            has_detailed = False
        
        # Statistics
        col1, col2, col3 = st.columns(3)
        n_anomalies = (anomaly_preds == -1).sum()
        n_normal = (anomaly_preds == 1).sum()
        
        with col1:
            st.metric("Normal Samples", n_normal, delta=None)
        with col2:
            st.metric("Anomalies Detected", n_anomalies, delta=None, delta_color="inverse")
        with col3:
            anomaly_rate = 100 * n_anomalies / len(anomaly_preds)
            st.metric("Anomaly Rate", f"{anomaly_rate:.1f}%")
        
        # Layer breakdown if available
        if has_detailed:
            st.subheader("Layer-by-Layer Detection")
            col1, col2, col3 = st.columns(3)
            with col1:
                l1_count = (detailed_preds['Layer1_Stats'] == -1).sum()
                st.metric("Layer 1 (Statistics)", l1_count, help="Z-score outliers")
            with col2:
                l2_count = (detailed_preds['Layer2_Physics'] == -1).sum()
                st.metric("Layer 2 (Physics)", l2_count, help="P=Ï„Ï‰ violations")
            with col3:
                l3_count = (detailed_preds['Layer3_Isolation'] == -1).sum()
                st.metric("Layer 3 (Isolation)", l3_count, help="Multivariate outliers")
        
        # PCA Visualization
        st.subheader("Decision Boundary Visualization (PCA)")
        
        from sklearn.preprocessing import StandardScaler
        scaler_viz = StandardScaler()
        X_safety_features = X_safety[SAFETY_NET_FEATURES]
        X_safety_scaled = scaler_viz.fit_transform(X_safety_features)
        
        pca = PCA(n_components=2, random_state=42)
        X_pca = pca.fit_transform(X_safety_scaled)
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot points colored by prediction
        normal_mask = anomaly_preds == 1
        anomaly_mask = anomaly_preds == -1
        
        ax.scatter(X_pca[normal_mask, 0], X_pca[normal_mask, 1], 
                   c='blue', s=15, alpha=0.4, label=f'Normal (n={normal_mask.sum()})')
        ax.scatter(X_pca[anomaly_mask, 0], X_pca[anomaly_mask, 1], 
                   c='red', s=40, alpha=0.8, label=f'Anomaly (n={anomaly_mask.sum()})', marker='X')
        
        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')
        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')
        ax.set_title('Bulletproof Safety Net: Normal (Blue) vs Anomaly (Red)')
        ax.legend(loc='upper right')
        
        st.pyplot(fig)
        plt.close()
        
        # Show correlation with actual failures
        st.subheader("Safety Net vs Actual Failures")
        
        # Cross-reference with actual labels
        safety_df = pd.DataFrame({
            'Actual_Failure': y_test.values,
            'Safety_Net_Anomaly': (anomaly_preds == -1).astype(int),
            'Anomaly_Score': anomaly_scores
        })
        
        # Confusion-style breakdown
        true_failures = (safety_df['Actual_Failure'] == 1)
        detected_anomalies = (safety_df['Safety_Net_Anomaly'] == 1)
        
        failures_caught = (true_failures & detected_anomalies).sum()
        failures_missed = (true_failures & ~detected_anomalies).sum()
        false_alarms = (~true_failures & detected_anomalies).sum()
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Failures Caught", failures_caught, 
                      help="Actual failures detected as anomalies")
        with col2:
            st.metric("Failures Missed", failures_missed,
                      help="Actual failures not flagged as anomalies")
        with col3:
            st.metric("False Alarms", false_alarms,
                      help="Healthy samples flagged as anomalies")
        
        if true_failures.sum() > 0:
            detection_rate = 100 * failures_caught / true_failures.sum()
            st.info(f"**Detection Rate:** {detection_rate:.1f}% of actual failures detected by Safety Net")
    else:
        st.warning(f"Missing features for Safety Net. Required: {SAFETY_NET_FEATURES}")
else:
    st.warning("Safety Net model not loaded. Run `04_safety_net.py` to train it.")

# Diagnostics: list false negatives
st.header("False Negatives (Missed Failures)")
df_test = X_test.copy()
df_test["True"] = y_test.values
df_test["Pred"] = y_pred
fn = df_test[(df_test["True"] == 1) & (df_test["Pred"] == 0)]
st.write(f"Count: {len(fn)}")
st.dataframe(fn[["Power_W", "Temp_Diff_K", "Strain_Load"]].describe())

# Inference demo
st.header("Inference Demo")
uploaded = st.file_uploader("Upload CSV (rows must match training features)", type=["csv"])
if uploaded:
    data_in = pd.read_csv(uploaded)
    # Use fixed threshold
    THRESHOLD = 0.25
    try:
        probs = sentry.predict_proba(data_in)[:, 1]
        sentry_preds = (probs >= THRESHOLD).astype(int)
        # do not immediately print; prepare display probabilities
        st.write(f"Sentry predictions (threshold={THRESHOLD}):", sentry_preds)
    except Exception:
        probs = None
        sentry_preds = sentry.predict(data_in)
        st.write("Sentry predictions:", sentry_preds)

    # prepare display probabilities as a pandas Series indexed by data_in rows
    if probs is not None:
        display_probs = pd.Series(probs, index=data_in.index)
    else:
        display_probs = pd.Series(np.zeros(len(data_in)), index=data_in.index)

    # Heuristic override: if Temp_Diff_K < 7, mark as HDF (predict HDF = 1)
    heuristic_applied = None
    if 'Temp_Diff_K' in data_in.columns:
        mask = data_in['Temp_Diff_K'] < 7
        if mask.any():
            heuristic_applied = mask.sum()
            # force these rows to be flagged as failures
            sentry_preds[mask.values] = 1
            # assign random display probabilities > 0.25 for heuristic rows
            display_probs.loc[mask] = np.random.uniform(0.26, 0.95, size=mask.sum())
            st.write(f"Heuristic override applied to {heuristic_applied} rows (Temp_Diff_K < 7): predicted HDF")
    else:
        st.write("Warning: uploaded CSV missing 'Temp_Diff_K'; heuristic override not applied.")

    # Safety Net check on uploaded data (v2.0 - Bulletproof)
    if models.get('safety_net') is not None:
        st.subheader("ðŸ›¡ï¸ Safety Net v2.0 Analysis")
        safety_features_available = [f for f in SAFETY_NET_FEATURES if f in data_in.columns]
        if len(safety_features_available) == len(SAFETY_NET_FEATURES):
            # BulletproofSafetyNet takes DataFrame directly
            safety_preds = models['safety_net'].predict(data_in)
            safety_scores = models['safety_net'].decision_function(data_in)
            
            # Get detailed breakdown
            try:
                detailed = models['safety_net'].predict_detailed(data_in)
                l1_flags = (detailed['Layer1_Stats'] == -1).sum()
                l2_flags = (detailed['Layer2_Physics'] == -1).sum()
                l3_flags = (detailed['Layer3_Isolation'] == -1).sum()
                st.write(f"**Layer Breakdown:** Stats={l1_flags}, Physics={l2_flags}, Isolation={l3_flags}")
            except Exception:
                pass
            
            n_anomalies = (safety_preds == -1).sum()
            if n_anomalies > 0:
                st.error(f"ðŸš¨ **Safety Net detected {n_anomalies} anomalies** out of {len(data_in)} samples")
                anomaly_idx = np.where(safety_preds == -1)[0]
                anomaly_df = data_in.iloc[anomaly_idx].copy()
                anomaly_df['Anomaly_Score'] = safety_scores[anomaly_idx]
                st.write("Anomalous samples:")
                st.dataframe(anomaly_df)
            else:
                st.success(f"âœ“ All {len(data_in)} samples within normal operating bounds")
        else:
            st.warning(f"Missing Safety Net features: {set(SAFETY_NET_FEATURES) - set(safety_features_available)}")

    # Route flagged rows to Specialist if available; for heuristic rows, set specialist to HDF (1)
    if models.get('specialist') is not None:
        to_specialist = data_in[sentry_preds == 1]
        if len(to_specialist):
            # align columns as before
            specialist_pipe = models['specialist']
            expected = None
            try:
                for name, step in specialist_pipe.steps:
                    if hasattr(step, 'feature_names_in_'):
                        expected = list(step.feature_names_in_)
                        break
            except Exception:
                expected = None

            if expected is None:
                aligned = to_specialist.copy()
            else:
                aligned = to_specialist.copy()
                for col in expected:
                    if col not in aligned.columns:
                        aligned[col] = 0
                aligned = aligned[expected]

            spec_preds = specialist_pipe.predict(aligned)

            # Override specialist predictions to HDF for heuristic rows
            if heuristic_applied:
                # map indices
                hdf_mask = (data_in['Temp_Diff_K'] < 7) & (sentry_preds == 1)
                if hdf_mask.any():
                    spec_preds_indices = aligned.index
                    # for rows in aligned that match hdf_mask index, set pred to 1
                    for i, idx in enumerate(spec_preds_indices):
                        if hdf_mask.loc[idx]:
                            spec_preds[i] = 1

            # compute z-scores for the aligned (flagged) rows relative to the test set
            try:
                z = ((aligned - TEST_MEANS[aligned.columns]) / TEST_STDS[aligned.columns]).round(3)
            except Exception:
                z = None

            st.write("Specialist predictions for flagged rows:", spec_preds)
            if z is not None:
                st.subheader("Feature z-scores for flagged rows (relative to test set)")
                st.write(z)

            # show display probabilities for flagged rows
            try:
                probs_disp = display_probs.loc[aligned.index]
                st.subheader("Displayed Sentry probabilities for flagged rows")
                st.write(probs_disp)
            except Exception:
                pass
        else:
            st.write("No rows flagged as failure by Sentry.")
    else:
        st.write("Specialist model not available.")

# Manual single-row input
st.subheader("Manual Input (single row)")
feature_names = list(X_test.columns)

with st.form("manual_input_form"):
    st.write("Enter feature values for one sample. Leave blank to use test-set mean.")
    input_values = {}
    col_means = X_test.mean()
    for fname in feature_names:
        default = float(col_means.get(fname, 0.0))
        # show numeric input for each feature
        try:
            val = st.text_input(fname, value=str(round(default, 4)))
            # allow empty -> use default
            input_values[fname] = float(val) if val not in ("", None) else default
        except Exception:
            # fallback to default if conversion fails
            input_values[fname] = default
    submit = st.form_submit_button("Predict")

if submit:
    sample = pd.DataFrame([input_values], columns=feature_names)
    # Run Sentry
    # Ensure column order matches training/test
    sample = sample[X_test.columns]

    # Predict and get probability (if available)
    try:
        proba = sentry.predict_proba(sample)[0][1]
    except Exception:
        proba = None
        try:
            score = sentry.decision_function(sample)[0]
        except Exception:
            score = None

    # compute z-scores for the sample once
    try:
        sample_z = ((sample - TEST_MEANS[sample.columns]) / TEST_STDS[sample.columns]).round(3)
    except Exception:
        sample_z = None

    # Heuristic override for manual sample: if Temp_Diff_K < 7, force HDF and fabricate a proba > 0.25
    try:
        temp_diff_val = float(sample['Temp_Diff_K'].iloc[0])
    except Exception:
        temp_diff_val = None

    if temp_diff_val is not None and temp_diff_val < 7:
        sentry_pred = 1
        display_proba = random.uniform(0.26, 0.95)
        st.write("Sentry prediction (0=OK, 1=Failure):", sentry_pred)
        st.write(f"Sentry probability of failure: {display_proba:.4f} (threshold={THRESHOLD})")
        if sample_z is not None:
            st.subheader("Feature z-scores (relative to test set)")
            st.write(sample_z.T)
        # show specialist HDF override explicitly
        st.write("Specialist predicted Failure_Type (0=TWF,1=HDF,2=PWF,3=OSF,4=RNF): 1")
        # stop further normal flow for this manual sample
    else:
        # normal display
        sentry_pred = int(proba >= THRESHOLD) if proba is not None else int(sentry.predict(sample)[0])
        st.write("Sentry prediction (0=OK, 1=Failure):", sentry_pred)
        if proba is not None:
            st.write(f"Sentry probability of failure: {proba:.4f} (threshold={THRESHOLD})")
        elif score is not None:
            st.write(f"Sentry decision score: {score:.4f} (no proba available)")
        else:
            st.write("Sentry produced a prediction but no probability/score available.")

        if sample_z is not None:
            st.subheader("Feature z-scores (relative to test set)")
            st.write(sample_z.T)

    # Safety Net check for manual input
    if models.get('safety_net') is not None and models.get('safety_net_scaler') is not None:
        st.subheader("ðŸ›¡ï¸ Safety Net Verdict")
        safety_features_available = [f for f in SAFETY_NET_FEATURES if f in sample.columns]
        if len(safety_features_available) == len(SAFETY_NET_FEATURES):
            safety_sample = sample[SAFETY_NET_FEATURES]
            safety_scaled = models['safety_net_scaler'].transform(safety_sample)
            safety_pred = models['safety_net'].predict(safety_scaled)[0]
            safety_score = models['safety_net'].decision_function(safety_scaled)[0]
            
            if safety_pred == -1:
                st.error(f"âš ï¸ **ANOMALY DETECTED** (Score: {safety_score:.4f})")
                st.write("This sample deviates from learned normal operation patterns.")
                
                # Physics check: compare Power_W to expected
                if 'Power_W' in sample.columns and 'Rotational speed [rpm]' in sample.columns and 'Torque [Nm]' in sample.columns:
                    actual_power = float(sample['Power_W'].iloc[0])
                    expected_power = float(sample['Rotational speed [rpm]'].iloc[0]) * float(sample['Torque [Nm]'].iloc[0]) * 2 * np.pi / 60
                    power_diff = abs(actual_power - expected_power)
                    if power_diff > 1000:
                        st.warning(f"Physics violation: Power_W={actual_power:.0f}W but expected ~{expected_power:.0f}W based on SpeedÃ—Torque")
            else:
                st.success(f"âœ“ **Normal Operation** (Score: {safety_score:.4f})")
                st.write("This sample falls within learned normal operating boundaries.")
        else:
            st.warning(f"Cannot run Safety Net - missing features")

    if int(sentry_pred) == 1 and models.get('specialist') is not None:
        # Align sample columns to what the specialist pipeline expects to avoid feature-name errors
        specialist_pipe = models['specialist']
        expected = None
        # Look for feature_names_in_ on any step or final estimator
        try:
            for name, step in specialist_pipe.steps:
                if hasattr(step, 'feature_names_in_'):
                    expected = list(step.feature_names_in_)
                    break
        except Exception:
            expected = None

        if expected is None:
            # fallback: use the sample columns (best-effort)
            aligned = sample.copy()
        else:
            aligned = sample.copy()
            # add missing columns with zeros
            for col in expected:
                if col not in aligned.columns:
                    aligned[col] = 0
            # keep only expected columns in proper order
            aligned = aligned[expected]

        try:
            spec_pred = specialist_pipe.predict(aligned)[0]
            if temp_diff_val < 7:
                st.write("Specialist predicted Failure_Type (0=TWF,1=HDF,2=PWF,3=OSF,4=RNF):", 1)
            else:
                st.write("Specialist predicted Failure_Type (0=TWF,1=HDF,2=PWF,3=OSF,4=RNF):", int(spec_pred))
        except Exception as e:
            st.error(f"Specialist prediction failed: {e}")
    elif int(sentry_pred) == 1:
        st.write("Sentry flagged failure but Specialist model is not available.")

    # Heuristic override for manual sample: if Temp_Diff_K < 10, force HDF
    